{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"HW02_gradient.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"cV9QUmabX0vc"},"source":["# 50 оттенков градиентного спуска \n","\n","В этом задании вам предстоит реализовать линейный классификатор и натренировать его, используя различные модификации градинетного спуска. Тетрадка позаимствована с [шадовского курса по нейронкам.](https://github.com/yandexdataschool/Practical_DL/blob/master/week01_backprop/adapdive_sgd/adaptive_sgd.ipynb)"]},{"cell_type":"code","metadata":{"id":"5_psCipcX0vh"},"source":["import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HP6VFuzTX0vi"},"source":["## Генерация выборки\n","\n","Для наших целей будем использовать искуственно сгенерированные данные."]},{"cell_type":"code","metadata":{"id":"dHlrtKsvX0vj"},"source":["from sklearn import datasets, preprocessing\n","\n","# keep random_state=42 for deterministic results\n","(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4, random_state=42)\n","ind = np.logical_or(y == 1, X[:, 1] > X[:, 0] - 0.5)\n","X = X[ind, :]\n","m = np.array([[1, 1], [-2, 1]])\n","X = preprocessing.scale(X)\n","y = y[ind]\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQuDdVtKX0vk"},"source":["### [1] Варка фичей\n","\n","Как вы можете заметить, данные не являются линейно разделимыми. Нам придётся добавить в обучающую выборку новые фичи либо использовать нелинейные модели. Предположим, что разделяющая поверхность имеет вид окружности. Добавьте в матрицу признаков дополнительные колонки $x_1^2$, $x_2^2$ и $x_1 \\cdot x_2$."]},{"cell_type":"code","metadata":{"id":"y9uwOl_UX0vk"},"source":["def expand(X):\n","    \"\"\"\n","    Добавляет квадратичные фичи. \n","    Для каждой строки матрицы находит строку \n","    [feature0, feature1, feature0^2, feature1^2, feature0*feature1, 1]\n","    \n","    :param X: матрица фичей, shape [n_samples,2]\n","    :returns: расширенная матрица фичей, shape [n_samples,6]\n","    \"\"\"\n","\n","    # ваш код здесь\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iimMENptX0vl"},"source":["### [3] Логистическая регрессия \n","\n","Для классификации будем использовать логистическую регрессию. \n","\n","$$ a(x; w) = \\langle w, x \\rangle $$\n","$$ P( y=1 \\; \\big| \\; x, \\, w) = \\dfrac{1}{1 + \\exp(- \\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle)$$\n"]},{"cell_type":"code","metadata":{"id":"qv8nmO9ZX0vl"},"source":["def probability(X, w):\n","    \"\"\"\n","    Принимает на вход матрицу фичей и вектор весов\n","    Возвращает предсказание вероятность того, что y = 1 при фиксированных x, P(y=1|x)\n","    \n","    :param X: расширенная матрица фичей [n_samples,6] (expanded)\n","    :param w: вектор весов [6]\n","    :returns: вектор вероятностей\n","    \"\"\"\n","\n","    # ваш код здесь"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPTy6FfbX0vm"},"source":["Для логистической регрессии оптимальный параметр находится минимизацией кросс-энтропии: \n","\n","$$ L(w) =  - {1 \\over \\ell} \\sum_{i=1}^\\ell \\left[ {y_i \\cdot log P(y_i = 1 \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i = 1 \\, | \\, x_i,w))}\\right] $$\n","\n"]},{"cell_type":"code","metadata":{"id":"2V2M5DQFX0vm"},"source":["def compute_loss(X, y, w):\n","    \"\"\"\n","    Принимает на вход матрицу весов, вектор ответов и вектор весов.\n","    Выдаёт на выход значение функции потерь, расчитанное по формуле выше.\n","    \"\"\"\n","\n","    # ваш код здесь"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--EpWK4wX0vm"},"source":["Мы будем обучать модель методом градиентного спуска. Для этого нам придётся вычислить градиент функции потерь, представленной выше. Возьмите листочек, ручку и в бой! \n","\n","$$ \\nabla_w L = ...$$\n","\n","Тут обойдёмся даже без матричного дифириенцирования. А вот в следущий раз его не миновать..."]},{"cell_type":"code","metadata":{"id":"WdUkkXTRX0vn"},"source":["def compute_grad(X, y, w):\n","    \"\"\"\n","    Нахоит значение градиента.\n","    \"\"\"\n","\n","    # ваш код здесь"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gllQDiV3X0vn"},"source":["Функция ниже предназначена для визуализации процесса обучения. "]},{"cell_type":"code","metadata":{"id":"sn_JspEKX0vn"},"source":["from IPython import display\n","\n","h = 0.01\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","def visualize(X, y, w, history):\n","    \"\"\"С помощью магии matplolib выдаёт красоты результатов классификации\"\"\"\n","    Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w)\n","    Z = Z.reshape(xx.shape)\n","    plt.subplot(1, 2, 1)\n","    plt.contourf(xx, yy, Z, alpha=0.8)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","    plt.xlim(xx.min(), xx.max())\n","    plt.ylim(yy.min(), yy.max())\n","    \n","    plt.subplot(1, 2, 2)\n","    plt.plot(history)\n","    plt.grid()\n","    ymin, ymax = plt.ylim()\n","    plt.ylim(0, ymax)\n","    display.clear_output(wait=True)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLel5ILSX0vo"},"source":["# убедитесь, что у вас она работает, запустив код ниже \n","# (он отработает если вы верно реализовали expend и probability)\n","dummy_weights = np.linspace(-1, 1, 6)\n","visualize(X, y, dummy_weights, [0.5, 0.5, 0.25])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uF3oox56X0vo"},"source":["## Обучение\n","\n","Пришло время обучить нашу модель. Для этого вам придётся дописать кусочки функций ниже. Обязательно попробуйте поменять гиперпараметры (размер батча и скорость обучения) и посмотреть как будет изменяться анимация. "]},{"cell_type":"markdown","metadata":{"id":"7wUZuvcCX0vp"},"source":["### [2] Mini-batch SGD\n","\n","Берём несколько рандомных наблюдений и ищем градиент по ним! \n","\n","$$ w_t = w_{t-1} - \\eta \\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n","\n"]},{"cell_type":"code","metadata":{"id":"Nk-PHGhdX0vp"},"source":["np.random.seed(42)\n","w = np.array([0, 0, 0, 0, 0, 1])\n","\n","eta= 0.1 \n","\n","n_iter = 100\n","batch_size = 4\n","loss = np.zeros(n_iter)\n","plt.figure(figsize=(12, 5))\n","\n","for i in range(n_iter):\n","    \n","    # Ваш код здесь \n","\n","visualize(X, y, w, loss)\n","plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wWDMw4teX0vp"},"source":["### [2] Momentum SGD\n","\n","Momentum это метод, который помогает стохастическому градиентному спуску сохранять направление движения. Это осуществляется за счёт добавления в выражение дополнительного слагаемого: накопленного за предыдущие шаги градиента с весом $\\alpha$. \n","<br>\n","<br>\n","\n","$$ \\nu_t = \\alpha \\nu_{t-1} + \\eta\\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n","$$ w_t = w_{t-1} - \\nu_t$$\n"]},{"cell_type":"code","metadata":{"id":"2fwnoqT3X0vq"},"source":["np.random.seed(42)\n","w = np.array([0, 0, 0, 0, 0, 1])\n","\n","eta = 0.05 \n","alpha = 0.9 \n","nu = np.zeros_like(w)\n","\n","n_iter = 100\n","batch_size = 4\n","loss = np.zeros(n_iter)\n","plt.figure(figsize=(12, 5))\n","\n","for i in range(n_iter):\n","    \n","    # Ваш код здесь \n","\n","visualize(X, y, w, loss)\n","plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"646odJCRX0vq"},"source":["### [2] RMSprop\n","\n","В этом блоке реализуем RMSprop. Эта вариация градиентного спуска позволяет изменять скорость обучения индивидуально для каждого параметра. \n","\n","$$ G_t^j = \\alpha G_{t-1}^j + (1 - \\alpha) g_{tj}^2 $$\n","$$ w_t^j = w_{t-1}^j - \\dfrac{\\eta}{\\sqrt{G_t^j + \\varepsilon}} g_{tj} $$"]},{"cell_type":"code","metadata":{"id":"05bkdh8CX0vq"},"source":["np.random.seed(42)\n","\n","w = np.array([0, 0, 0, 0, 0, 1.])\n","\n","eta = 0.1 \n","alpha = 0.9 \n","g2 = np.zeros_like(w)\n","eps = 1e-8\n","\n","n_iter = 100\n","batch_size = 4\n","loss = np.zeros(n_iter)\n","plt.figure(figsize=(12,5))\n","for i in range(n_iter):\n","\n","    # Ваш код здесь \n","\n","visualize(X, y, w, loss)\n","plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYy6J7kwX0vr"},"source":["Как траектории обучения различных вариаций градиентного спуска различаются между собой? Ожидаемо ли это? Почему? Что нужно сделать, чтобы реализовать Adam? "]},{"cell_type":"markdown","metadata":{"id":"6DLxlyX0X0vr"},"source":["### [2] За каждую адекватную вариацию\n","\n","Если понравилось реализовывать свои градиентные спуски и ты находишься от них под глубоким впечатлением, предлагается реализовать Adam за дополнительные баллы. "]},{"cell_type":"code","metadata":{"id":"KXKWmTRKX0vs"},"source":["# Ваш код здесь "],"execution_count":null,"outputs":[]}]}