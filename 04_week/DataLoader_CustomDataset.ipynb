{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "DataLoader - CustomDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9KmAXmXHe3l"
      },
      "source": [
        "# DataLoader in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaiKaG5KHu8B"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvnsxJbxH0Bp"
      },
      "source": [
        "%cd /content/gdrive/My Drive/HSE_DL_2021/04_week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xRJejjSHe3q"
      },
      "source": [
        "__DataLoader__ -- класс в PyTorch, который позволяет итеративно проходить по датасету, отвечает за оркестрацию всего процесса работы с датасетом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXLdBjXHqZ_"
      },
      "source": [
        "from IPython import display\n",
        "display.Image('images/DataLoaderParams.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1jMBH-He3r"
      },
      "source": [
        "- __dataset__ -- позволяет создать кастомные классы для работы с датасетом, где можно указать логику формирвоания батча.\n",
        "- __sampler__ -- определяет порядок элементов из датасета, которые будут идти в батч, то есть это список индексов, объединенных в батч. Удобно переопределять, когда обучение распредленное.  \n",
        "- __collate_fn__ -- позволяет сделать финальную предобработку над батчем данных. Если, например, в батч попали последовательности разных размеров, то после уже сбора батча, можно будет дополнить последовательности нулями относительно максимально длиной последовательности.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvXSQcrYHe3s"
      },
      "source": [
        "## Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M50Y25tIHe3s"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from torch.utils.data.dataloader import default_collate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on7diefvHe3u"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2uMPntFfBgD"
      },
      "source": [
        "with open('data/X_train_cat.pickle', 'rb') as f:\n",
        "    X, target = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUtWlc3aHe3u"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    # Конструктор, где считаем датасет\n",
        "    def __init__(self, dataset_path):\n",
        "        with open(dataset_path, 'rb') as f:\n",
        "            self.X, self.target = pickle.load(f)\n",
        "\n",
        "        return\n",
        "    \n",
        "    # Переопределяем метод вычисление размера датасета\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # Переопределяем метод,\n",
        "    # который достает по индексу наблюдение из датасет\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.target[idx]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W5WZfKhHe3v"
      },
      "source": [
        "## Custom Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbWRfZacNRru"
      },
      "source": [
        "a = [1,2,3,4,5, 6, 7, 8]\n",
        "print(np.random.permutation(a))\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhvQ2ucHe3v"
      },
      "source": [
        "class CustomSampler(Sampler):\n",
        "\n",
        "    # Конструктор, где инициализируем индексы элементов\n",
        "    def __init__(self, data):\n",
        "        self.data_indices = np.random.permutation(len(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_indices)\n",
        "\n",
        "    # Возращает итератор,\n",
        "    # который будет возвращать индексы из перемешанного датасета\n",
        "    def __iter__(self):\n",
        "        return iter(self.data_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVBYdnW7He3w"
      },
      "source": [
        "## Custom Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhBOBm6HHe3w"
      },
      "source": [
        "def collate(batch):\n",
        "    return default_collate(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb0SivIAHe3x"
      },
      "source": [
        "def create_data_loader(train_dataset, train_sampler,\n",
        "                       test_dataset, test_sampler):\n",
        "    train_loader = DataLoader(dataset=train_dataset, sampler=train_sampler,\n",
        "                              batch_size=BATCH_SIZE, collate_fn=collate,\n",
        "                              shuffle=False)\n",
        "\n",
        "    test_loader = DataLoader(dataset=test_dataset, sampler=test_sampler,\n",
        "                             batch_size=BATCH_SIZE, collate_fn=collate,\n",
        "                             shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhJ90vhEHe3x"
      },
      "source": [
        "# Создаем объекты Custom Dataset и Sampler\n",
        "train_ds = CustomDataset('data/X_train_cat.pickle')\n",
        "train_sampler = CustomSampler(train_ds.X)\n",
        "\n",
        "test_ds = CustomDataset('data/X_test_cat.pickle')\n",
        "test_sampler = CustomSampler(test_ds.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1N61mcsHe3y"
      },
      "source": [
        "train_loader, test_loader = create_data_loader(train_ds, train_sampler,\n",
        "                                               test_ds, test_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNYKpbmuHe3y"
      },
      "source": [
        "def run_train():\n",
        "    print('Run train')\n",
        "    for epoch in range(EPOCHS):\n",
        "        for features, labels in train_loader:\n",
        "            print(features.shape, labels.shape)\n",
        "            break\n",
        "\n",
        "        # Run validation\n",
        "        print('Run validation')\n",
        "        for features, labels in test_loader:\n",
        "            print(features.shape, labels.shape)\n",
        "            break\n",
        "            \n",
        "        break\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk_F_6rEHe3z"
      },
      "source": [
        "run_train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "urW-3y7-BJx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}