{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "PyTorch.Graph.AutoGrad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSBQQfSPZ_y1"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwQwGsqTr9ug"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TljUKUrsEXT"
      },
      "source": [
        "%cd /content/gdrive/My Drive/HSE_DL_2021/03_week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahbw4PH_a8iJ"
      },
      "source": [
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94djClMiZ_zD"
      },
      "source": [
        "## Dynamic Computational Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H067mYIZ_zE"
      },
      "source": [
        "После того, как были реализованы архитектура модели и весь процес обучения и валидация сети, при запуске кода в PyTorch происходят следующие этапы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGQGs1JSZ_zE"
      },
      "source": [
        "1. Строится вычислительный граф (направленный ациклический граф), где каждый узел -- это тензор, а ребро, ведущее к дргуому узлу, это выполнение операции над данным тензором, которое ведет к результату - другому тензору."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJA0oPWEbBWW"
      },
      "source": [
        "display.Image('images/Graph.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrpldhcZ_zE"
      },
      "source": [
        "Реализуем двухслойную сеть для задачи регрессии. И граф для такой архитектуры бдует выглядить следующим образом:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1GiZ7JQsWDm"
      },
      "source": [
        "display.Image('images/RegGraph.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# how to combine both lines: CPU and GPU\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') "
      ],
      "metadata": {
        "id": "b92iWp3Z-d2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvitFBVxZ_zF"
      },
      "source": [
        "batch_size = 64\n",
        "input_size = 3\n",
        "hidden_size = 2\n",
        "output_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNtyFK3BZ_zF"
      },
      "source": [
        "# Create random input and output data\n",
        "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
        "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = torch.mm(x, w1)  # another way x @ w1\n",
        "    h_relu = h.clamp(min=0)\n",
        "\n",
        "    y_pred = torch.mm(h_relu, w2)  # h_relu @ w2\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NpXMA94oRfi"
      },
      "source": [
        "x.shape, w1.shape, w2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqcbtSOuZ_zG"
      },
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4NWjNzAZ_zG"
      },
      "source": [
        "2. Еще одно фундаментальное понятие и важный элемент при построении графа -- это [__Autograd__](https://pytorch.org/docs/stable/notes/autograd.html) -- автоматическое дифференцирование.\n",
        "\n",
        "Для того чтобы с помощью стохастического градиентного спуска обновить обучаемые параметры сети, нужно посчитать градиенты. И как известно, обновление весов, которые учавтсвуют в нескольких операциях, происходит по `правилу дифференцирования сложной функции` (цепное правило или __chain rule__)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AVQ8fWR4UNh"
      },
      "source": [
        "$h_{relu} = ReLU(XW_1)$ \\\n",
        "\n",
        "$y_{pred} = h_{relu} \\cdot W_2$ \\\n",
        "\n",
        "$L = (y_{pred}-y)^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idc96HK1tq6g"
      },
      "source": [
        "display.Image('images/RegChainRule.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzPc3417Z_zH"
      },
      "source": [
        "То есть (1) вычислительный граф позволяет определить последовательность операций, а (2) автоматическое дифференцирование посчитать нужные градиенты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sH0UKCaZ_zH"
      },
      "source": [
        "Если бы `Autograd` не было, то тогда backprop надо было бы реализовывать самим, и как это бы выглядело?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMk4Gcz0Z_zI"
      },
      "source": [
        "Рассмотрим на примере, как посчиать градиенты для весов из входного слоя, где входной вектора `X` состоит из 3-х компонент. А входной слой вторую размерность имеет равной 2. \n",
        "\n",
        "После чего это идет в `ReLU`, но для простоты опустим на время ее, и посмотрим как дальше это идет по сети.\n",
        "\n",
        "Ниже написано, как это все вычисляется и приводит нас к значению целевой функции для одного наблюдения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfGT5_F2t7XF"
      },
      "source": [
        "display.Image('images/1.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHZ3zKhkZ_zI"
      },
      "source": [
        "Тогда, чтобы посчитать градиент по первому элементу из обучаемой матрицы на первом слое, необходимо взять производоную у сложной функции. А этот как раз делается по `chain rule`: сначала берем у внешней, потом спускаемся на уровень ниже, и так пока не додйдем до то функции, после которой эта перменная уже нигде не участвует:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yXK9076uHUc"
      },
      "source": [
        "display.Image('images/2.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn7w7QnzZ_zK"
      },
      "source": [
        "Перепишем это все в матричном виде, то есть сделаем аналог вида матрицы весов из первого слоя, но там уже будут её градиенты, котоыре будут нужны чтобы как раз обновить эти веса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__nUL_fuMPc"
      },
      "source": [
        "display.Image('images/3.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkNPvLOqZ_zN"
      },
      "source": [
        "Как видно, здесь можно вектор X вынести, то есть разделить на две матрицы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgc_r4e2wVpX"
      },
      "source": [
        "display.Image('images/4.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVEn-MtKZ_zO"
      },
      "source": [
        "То есть уже видно, что будем траспонировать входной вектор(матрицу). Но надо понимать, что в реальности у нас не одно наблюдение в батче, а несколько, тогда запись немного изменит свой вид:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFlyGX0RwZFI"
      },
      "source": [
        "display.Image('images/5.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKZcW48XZ_zO"
      },
      "source": [
        "Теперь мы видим, как на самом деле вычисляется вот те самые частные производные для вектора X, то есть видно, как математически это можно записать, а именно:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MQzrYhww30q"
      },
      "source": [
        "display.Image('images/6.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dv90_Aww6vv"
      },
      "source": [
        "display.Image('images/7.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHWJq4NjcplX"
      },
      "source": [
        "**Вопрос на понимание формул:** почему на картинке выше **$x^T$** транспонирован?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t22Rqn4_Z_zQ"
      },
      "source": [
        "Уже можно реализовать. Понятно, что транспонируется, что нет, и что на что умножается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y2-OQYxZ_zQ"
      },
      "source": [
        "Но помним про ReLU. Для простоты опустили, но теперь её учесть будет легче. \n",
        "\n",
        "Так как после первого слоя идет ReLU, а значит, занулились те выходы первого слоя, которые были __меньше__ нуля. Получается, что во второй слой не все дошло, тогда нужно обнулить, что занулил ReLU. \n",
        "\n",
        "Что занулил ReLU, мы можем выяснить при `forward pass`, а где именно поставить нули, то надо уже смотреть относительно `backward propagation`, на том выходе, где последний раз участвовал выход после ReLU, то есть:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGpgAr5sxASE"
      },
      "source": [
        "display.Image('images/8.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qaEEJc2Z_zR"
      },
      "source": [
        "Теперь реализуем эти формулы на PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random input and output data\n",
        "x = torch.randn(batch_size, input_size, device=device, dtype=dtype, requires_grad=True)\n",
        "y = torch.randn(batch_size, output_size, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "\n",
        "# One Forward pass: compute predicted y\n",
        "h = torch.mm(x, w1)  # another way x @ w1\n",
        "\n",
        "h_relu = h.clamp(min=0)\n",
        "\n",
        "y_pred = torch.mm(h_relu, w2)  # h_relu @ w2\n",
        "\n",
        "loss = (y_pred - y).pow(2).sum()"
      ],
      "metadata": {
        "id": "34yV9NKTDtcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL5o3yo1Z_zR"
      },
      "source": [
        "# TODO\n",
        "grad_w2 = None\n",
        "\n",
        "# TODO\n",
        "grad_w1 = None\n",
        "\n",
        "# # Update weights using gradient descent\n",
        "# w1 -= learning_rate * grad_w1\n",
        "# w2 -= learning_rate * grad_w2\n",
        "\n",
        "loss.backward()\n",
        "print('Gradient for w1 is calculated correctly:', (w1.grad == grad_w1).all().item())\n",
        "print('Gradient for w2 is calculated correctly:', (w2.grad == grad_w2).all().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eo63N54Z_zR"
      },
      "source": [
        "Благодаря `Autograd` реализацию `chain rule` можно избежать, так как для более сложных нейронных сетей вручную такое реализовать сложно, при этом сделать это эффективным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYGNKQ-CZ_zR"
      },
      "source": [
        "Для того чтобы PyTorch понял, за какими переменными надо \"следить\", то есть указать, что именно \"эти\" переменные являются обучаемыми, необходимо при создании тензора в качестве аттрибута указать __requires_grad=True__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUknutaxZ_zS"
      },
      "source": [
        "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQShRUNHZ_zS"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "for t in range(5000):\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    loss = (y_pred - y).pow(2).mean()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    # Теперь подсчет градиентов для весов происходит при вызове backward\n",
        "    loss.backward()\n",
        "   \n",
        "    # Обновляем значение весов, но укзаываем, чтобы PyTorch не считал эту операцию, \n",
        "    # которая бы учавствовала бы при подсчете градиентов в chain rule\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "        \n",
        "        # Теперь обнуляем значение градиентов, чтобы на следующем шаге \n",
        "        # они не учитывались при подсчете новых градиентов,\n",
        "        # иначе произойдет суммирвоание старых и новых градиентов\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "\n",
        "        # optimizer = Adam(model.parameters(), lr)\n",
        "        # optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTUrbi6zwjG"
      },
      "source": [
        "w1.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63hWrm3Z_zS"
      },
      "source": [
        "Осталось еще не вручную обновлять веса, а использовать адаптивные методы градинетного спсука. Для этого нужно использовать модуль __optim__. А помимо оптимайзера, еще можно использовать готовые целевые функции из модлуя __nn__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4EhFl4MZ_zT"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.Adam([w1, w2], lr=learning_rate)\n",
        " \n",
        "for t in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "    \n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    loss.backward()\n",
        "   \n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB_qJHeVZ_zT"
      },
      "source": [
        "После того, как мы сделали backward, в этот момент посчитались градиенты и граф уничтожился, то есть стёрлись все пути, которые связывали тензоры между собой. Это значит, что еще раз backward сделать не поулчится, будет ошибка. Но если вдруг нужно считать градиенты еще раз, то нужно при вызове backward задать `retain_graph=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kpomjnFZ_zT"
      },
      "source": [
        "Еще важный аттрибут, который есть у Tensor -- это `grad_fn`. В этом аттрибуте указывается та функция, посредством которой был создан этот тензор. Так PyTorch понимает, как именно считать по нему градиент."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MrHGt4zZ_zT"
      },
      "source": [
        "y_pred.grad_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Zz0tlIZ_zU"
      },
      "source": [
        "Также можно контролировать, должны ли градиенты течь или нет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AljUMbdgZ_zU"
      },
      "source": [
        "x = torch.tensor([1.], requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    with torch.enable_grad():\n",
        "        y = x * 2\n",
        "y.requires_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd3wk3HZZ_zU"
      },
      "source": [
        "## Почему Backprop надо понимать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6KPZfg0Z_zU"
      },
      "source": [
        "1. Backprop позволяет понимать, как те или иные операции, сложные конструкции в сети влияют на обнолвение весов.\n",
        "Почему лучше сделать конкатенацию тензоров, а не поэлементное сложение. Для этого нужно посмотреть на backprop, как будут обновляться веса.\n",
        "\n",
        "2. Даже на таком маленьком пример двуслойной MLP можно уже увидеть, когда `ReLU`, как функция активация, не очень хорошо применять. Если разреженные данные, то получить на выходе много нулей вероятнее, чем при использовании `LeakyReLU`, то есть градиенты будут нулевыми и веса никак не будут обновляться => сеть не обучается!\n",
        "\n",
        "3. В архитектуре могут встречаться недифференцируемые операции, и первое - это нужно понять, потому что при обучении сети это может быть не сразу заметно, просто качество модели будет плохое, и точность хорошую не поулчится достичь.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPD9J_XPZ_zW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}