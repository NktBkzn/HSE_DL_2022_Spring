{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1trn9F4vHX9"
   },
   "source": [
    "# Классификация и градиентные спуски\n",
    "\n",
    "В этой тетрадке мы попробуем немного посмотреть на то, как работают разные градиентные спуски. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcgDcJOovHYB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings         # чтобы никто не мешал бесчинствам с кодом\n",
    "warnings.filterwarnings(\"ignore\") # обработка всевозсожных warnings путем их игнорирования\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQe8vfswv2YM"
   },
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    ROOT = Path(\"/content/drive/My Drive/HSE_DL_2021/02_week/\")\n",
    "    \n",
    "    assert ROOT.is_dir(), \"Have you forgotten to 'Add a shortcut to Drive'?\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(str(ROOT))\n",
    "else:\n",
    "    ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIsyhmUjvHYC"
   },
   "source": [
    "# 1. Выборка\n",
    "\n",
    "Делать всё это мы будем на животных. Ежегодно около 7.6 миллионов бедных животных в США оказываются в приютах. Часть из них находит себе новую семью, часть возвращается к старому (бывает, что питомец потерялся и его нашли на улице), а часть погибает. Ужегодно усыпляется около 2.7 млн. собак и кошек.  \n",
    "\n",
    "Используя датасет с входной информацией (цвет, пол, возраст и т.п.) из одного из приютов, мы попытаемся спрогнозировать что произойдёт с новыми животными, которые попадут в этот приют. Данные, используемые в тетрадке уже были предварительно обработаны и приведены в удобную для построения моделей форму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA2TPHo4vHYD"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(f'{ROOT}/X_cat.csv', sep = '\\t', index_col=0)\n",
    "target = pd.read_csv(f'{ROOT}/y_cat.csv', sep = '\\t', index_col=0,  names=['status'])\n",
    "\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5ZSefafvHYE"
   },
   "source": [
    "В датасете находится около 27 тысяч наблюдений и 39 регрессоров. Посмотрим на то как выглядит распределение того, что произошло со зверятами по особям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IptC3WKvHYF"
   },
   "outputs": [],
   "source": [
    "target.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9bQ0IkjvHYF"
   },
   "source": [
    "Видим, что классы несбалансированы. Попробуем оставит четыре класса и объединить класс умерших животных с классом животных, которых усыпили. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21PcHtlSvHYG"
   },
   "outputs": [],
   "source": [
    "target = target.values\n",
    "target[target == 'Died'] = 'Euthanasia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeP0Lhe2vHYG"
   },
   "source": [
    "Закодируем классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oWokBTsvHYG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmfsvLULvHYH"
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otLvFCPavHYI"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0TQR8_evHYI"
   },
   "source": [
    "Разобьём выборку на тренировочную и тестовую. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDNHmr36vHYI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9IYETYPvHYJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UGwCG70vHYJ"
   },
   "source": [
    "# 2. Архитектурка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZTyzK4Py50b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn            # содержит функции для реалзации архитектуры нейронных сетей\n",
    "import torch.nn.functional as F  # содержит различные функции активации и не только\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezoTBX73vHYK"
   },
   "source": [
    "Функция для рисования графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SXl9eDVvHYK"
   },
   "outputs": [],
   "source": [
    "def plot(histories):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    for name, val_loss in histories:\n",
    "        plt.plot(val_loss, label=name)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DDTRshI22Ho"
   },
   "source": [
    " Соберем архитектуру модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV4QLIVv0r0_"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 37\n",
    "HIDDEN_SIZE = 25\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 105\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def build_model():\n",
    "    model = nn.Sequential(  \n",
    "        # Добавляем в нашу модель первый слой из 25 нейронов\n",
    "        nn.Linear(in_features=INPUT_SIZE, out_features=HIDDEN_SIZE),\n",
    "        nn.Sigmoid(),\n",
    "        \n",
    "        # Добавляем ещё один слой из 25 нейронов\n",
    "        nn.Linear(in_features=HIDDEN_SIZE, out_features=HIDDEN_SIZE),\n",
    "        nn.Sigmoid(),\n",
    "        \n",
    "        # Выходной вектор на количество классов, получаем с помощью такого же линейного приеобразования,\n",
    "        # как и предыдущие слои, но уже на нужное количество выходных нейронов (т.е. классов)\n",
    "        nn.Linear(in_features=HIDDEN_SIZE, out_features=OUTPUT_SIZE),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7nzgau33Avb"
   },
   "source": [
    "Напишем функцию, которая будет тренировать нейронную сеть. Функция имеет 2 логические части: первая - тренировочная, вторая - валидационная, другими словами, та, на которой вычисляется ошибка и сравнивается с ошибкой на тренировочных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRn2R0KJ1Kmp"
   },
   "outputs": [],
   "source": [
    "def run_train(model, optimizer, criterion, scheduler=None):\n",
    "    train_loss_values = []\n",
    "    train_accuracy_values = []\n",
    "    valid_loss_values = []\n",
    "    valid_accuracy = []\n",
    "    lr_history = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = []\n",
    "        running_acc = []\n",
    "        for features, label in train_loader:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # run model on the chosen batch\n",
    "            output = model(features)\n",
    "\n",
    "            # Calculate error and backpropagate\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # manual accuracy calculation; no torch lightning\n",
    "            acc = (output.argmax(dim=1)==label).sum() / len(label)\n",
    "\n",
    "            # Update weights with gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_acc.append(acc)\n",
    "\n",
    "        train_loss_values.append(np.mean(running_loss))\n",
    "        train_accuracy_values.append(np.mean(running_acc))\n",
    "        if epoch % 20 == 0:\n",
    "            print('EPOCH %d,  train_loss: %f, train_accuracy: %f' % (epoch, train_loss_values[-1], train_accuracy_values[-1]))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        # Run validation\n",
    "        running_loss = []\n",
    "        running_acc = []\n",
    "        with torch.no_grad(): # in validation loop we do not need gradients calculation; so switch it off\n",
    "            for features, label in test_loader:\n",
    "                output = model(features)\n",
    "                \n",
    "                # Calculate error ana accuracy\n",
    "                loss = criterion(output, label)\n",
    "                acc = (output.argmax(dim=1)==label).sum() / len(label)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "                running_acc.append(acc)\n",
    "\n",
    "            valid_loss_values.append(np.mean(running_loss))\n",
    "            valid_accuracy.append(np.mean(running_acc))\n",
    "            if epoch % 20 == 0:\n",
    "                print('EPOCH %d, valid_loss: %f, valid_accuracy: %f' % (epoch, valid_loss_values[-1], valid_accuracy[-1]))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # Decay Learning Rate\n",
    "            scheduler.step()\n",
    "            lr_history.append(scheduler.get_last_lr())\n",
    "\n",
    "    if scheduler is not None:\n",
    "        return train_loss_values, train_accuracy_values, valid_loss_values, valid_accuracy, lr_history\n",
    "        \n",
    "    return train_loss_values, train_accuracy_values, valid_loss_values, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q27i0kP3a0g"
   },
   "source": [
    "Создадим PyTorch `Dataloader`. Объект DataLoader принимает на вход датасет и ряд параметров, задающих процедуру формирования батча данных.\n",
    "\n",
    "Основным входным параметром `Dataloader` является объект PyTorch `Dataset`. Он нужен для доступа к элементы данных по конкретному индексу. \n",
    "\n",
    "На первый взгляд такая конструкция может показаться странной и излишней. Однако к ней просто нужно привыкнуть и со временем станет понятно насколько она удобна и лаконична. В силу своей гибкости она позволяет формировать размер и структуру батча по желанию, а также собирать батч из заранее выбранных элементов данных. Такая кастомная сборка батча может пригодиться, например, при работе с последовательными данными, такими как предложения в тексте, имеющими разную длину. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpRy2C4V3a_B"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(X_train, y_train, X_test, y_test):\n",
    "    train_tensor = data_utils.TensorDataset(torch.tensor(X_train.astype(np.float32)), torch.tensor(y_train))\n",
    "    train_loader = data_utils.DataLoader(dataset=train_tensor,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "\n",
    "    test_tensor = data_utils.TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(y_test))\n",
    "    test_loader = data_utils.DataLoader(dataset=test_tensor,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = create_data_loader(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py6-_0wpvHYN"
   },
   "source": [
    "# 3. Оптимизация \n",
    "\n",
    "### SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBiUj5TysK_v"
   },
   "outputs": [],
   "source": [
    "model1 = build_model()\n",
    "list(model1.parameters())[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGFgUTyuvHYO"
   },
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model1 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "sgd = optim.SGD(model1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# инициализируем Loss function (функцию потерь)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values, train_accuracy_values, valid_loss_values, valid_accuracy = run_train(model1, optimizer=sgd, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp3ghesy9JWb"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_5OGJF4vHYP"
   },
   "source": [
    "### Nesterov Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVwA2M_pvHYP"
   },
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model2 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "sgd = optim.SGD(model2.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values2, train_accuracy_values2, valid_loss_values2, valid_accuracy2 = run_train(model2, optimizer=sgd, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA5eiW_--AN5"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values), \n",
    "      ('momentum nesterov SGD', valid_loss_values2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcfJX5jPvHYQ"
   },
   "source": [
    "### RMSprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud8z7qOSvHYQ"
   },
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model3 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "rmsprop = optim.RMSprop(model3.parameters(), lr=LEARNING_RATE, alpha=0.9, eps=1e-08)\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values3, train_accuracy_values3, valid_loss_values3, valid_accuracy3 = run_train(model3, optimizer=rmsprop, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FjU0em9_X2g"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values), \n",
    "      ('momentum nesterov SGD', valid_loss_values2),\n",
    "      ('RMSprop', valid_loss_values3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZetN13i5vHYS"
   },
   "source": [
    "### Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxjzA6G5vHYT"
   },
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model4 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "adam = optim.Adam(model4.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values4, train_accuracy_values4, valid_loss_values4, valid_accuracy4 = run_train(model4, optimizer=adam, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmhZJRiSAKDd"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values), \n",
    "      ('momentum nesterov SGD', valid_loss_values2),\n",
    "      ('RMSprop', valid_loss_values3),\n",
    "      ('ADAM', valid_loss_values4)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsIQGhJyvHYV"
   },
   "source": [
    "# 4. Стратегии с постепенным понижением lr \n",
    "\n",
    "![](https://raw.githubusercontent.com/FUlyankin/neural_nets_econ/master/2019/sem_2/ahaha.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZLkoUa6EQ_n"
   },
   "outputs": [],
   "source": [
    "# функция для картинок, чтобы видеть как скорость обученя меняется от эпохи к эпохе\n",
    "def plot_learning_rate(lr_history):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1, EPOCHS+1), lr_history, label='learning rate')\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.xlim([1, EPOCHS+1])\n",
    "    plt.ylabel(\"learning rate\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Learning rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DORJkuW4vHYW"
   },
   "source": [
    "Стартовую скорость обучения специально берём большой! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coV4-vvjvHYW"
   },
   "outputs": [],
   "source": [
    "INIT_LR = 0.1  # берём lr намеренно большим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZG_eCMmHroe"
   },
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-l0AcAMOvHYX"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# Первая простенькая моделька \n",
    "model5 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "adam = optim.Adam(model5.parameters(), lr=INIT_LR, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# инициализируем \n",
    "scheduler = StepLR(adam, step_size=50, gamma=0.5)\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values5, train_accuracy_values5, valid_loss_values5, valid_accuracy5, lr_history5 = run_train(model5, optimizer=adam, \n",
    "                                                                                                        criterion=criterion, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPcjsNn2IOaE"
   },
   "outputs": [],
   "source": [
    "adam = optim.SGD(model5.parameters(), lr=INIT_LR)\n",
    "scheduler = StepLR(adam, step_size=2, gamma=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i+1, scheduler.get_last_lr())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvz8awtXvHYX"
   },
   "outputs": [],
   "source": [
    "# Смотрим как скорость обучения по немного понижалась\n",
    "plot_learning_rate(lr_history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTAlcNyLMlO4"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values), \n",
    "      ('momentum nesterov SGD', valid_loss_values2),\n",
    "      ('RMSprop', valid_loss_values3),\n",
    "      ('ADAM', valid_loss_values4),\n",
    "      ('ADAM_step', valid_loss_values5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6Wi1Va3vHYY"
   },
   "source": [
    "Попробуем ещё вариант!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmpv93IxvHYZ"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "# Первая простенькая моделька \n",
    "model6 = build_model()\n",
    "\n",
    "# инициализируем SGD optimizer\n",
    "adam = optim.Adam(model6.parameters(), lr=INIT_LR, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# инициализируем \n",
    "scheduler = MultiStepLR(adam, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "# запускаем процесс обучения\n",
    "train_loss_values6, train_accuracy_values6, valid_loss_values6, valid_accuracy6, lr_history6 = run_train(model6, optimizer=adam, \n",
    "                                                                                                        criterion=criterion, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n43RxrQIOeoE"
   },
   "outputs": [],
   "source": [
    "# Смотрим как скорость обучения по немного понижалась\n",
    "plot_learning_rate(lr_history6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50qD4pmKvHYZ"
   },
   "outputs": [],
   "source": [
    "plot([('Simple SGD', valid_loss_values), \n",
    "      ('momentum nesterov SGD', valid_loss_values2),\n",
    "      ('RMSprop', valid_loss_values3),\n",
    "      ('ADAM', valid_loss_values4),\n",
    "      ('ADAM_step', valid_loss_values5),\n",
    "      ('ADAM_step 2', valid_loss_values6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB5h35HRO5XI"
   },
   "source": [
    "Существует множество различных Scheduler`ов. Полный список можно найти на [странице](https://pytorch.org/docs/stable/optim.html) в разделе \"How to adjust learning rate\"\n",
    "\n",
    "Великолепное описание различных Scheduler`ов и как их применять можно найти по [ссылке](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goMH1Np-vHYZ"
   },
   "source": [
    "## Авторские права и почиташки \n",
    "\n",
    "* Для создания тетрадки использовался [вот этот мануал](https://github.com/sukilau/Ziff-deep-learning/blob/master/3-CIFAR10-lrate/CIFAR10-lrate.ipynb), адаптированный под PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PNGBENzvHYZ"
   },
   "source": [
    "##   Поиграться при желании\n",
    "Пришло время заняться исследованиями! На лекции мы с вами обсудили, что сегодня люди ставят довольно большое число разных экспериментов с циклической скоростью обучения. Делают они это, чтобы как-то соскальзывать с сёдел и выбираться из локальных минимумов.  В этом задании вам надо будет немного поэкспериментировать с такими скоростями обучения. Предлагается поизучать [Scheduler`ы](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), реализованные в PyTorch и выбрать 2-3 для сравнения с рассмотренными вариантами на семинаре\n",
    "\n",
    "Вообще поощряются любые эксперименты. Не забывайте строить картинки. Можете как-то видоизменить архитектуру сетки. Например, добавить какие-то новые слои или сделать её глубже. Эксперименты можно оформлять прямо в этой тетрадке. Её же и присылайте.\n",
    "\n",
    "Если хочется вдохновения, [в этой статье](https://www.jeremyjordan.me/nn-learning-rate/) можно найти много разных вариантов пересчёта скорости обучения. Есть варианты с циклами и даже косинусами! Единственное, что код в статье написан на Keras, но это никак не мешает изучить суть алгоритмов. Еще одна полезная [ссылка](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/), которую я уже приводил выше\n",
    "\n",
    "Плодотворных экспериментов :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBEKcQ1fvHYa"
   },
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_SGD_seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
