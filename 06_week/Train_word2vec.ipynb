{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Train_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CggdAnzJ2726"
      },
      "source": [
        "# https://stackoverflow.com/questions/47805170/whats-the-hardware-spec-for-google-colaboratory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qr3Pkpk288l"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ehj-jq29EB"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxtNczuR29K-"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNSg8pG1X5Rb"
      },
      "source": [
        "# Как научить компьютер читать? \n",
        "\n",
        "В этой тетрадке мы обучим свой собственный word2vec. Делать мы это будем на каком-нибудь не очень большом тексте, который вам предстоит выбрать самому. На выбор есть [несколько сказок](https://github.com/nevmenandr/word2vec-russian-novels/tree/master/vector-school) и других [литературных штук](https://github.com/nevmenandr/word2vec-russian-novels/tree/master/books_before) из школьной программы. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8-koqyX5Rf"
      },
      "source": [
        "# Ссылка на выбранное вами произведение\n",
        "# Я взял преступление и наказание\n",
        "url = 'https://raw.githubusercontent.com/nevmenandr/word2vec-russian-novels/master/books_before/CrimeAndPunishment.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg0ui61mX5Ri"
      },
      "source": [
        "Спарсим текст из файлика.\\\n",
        "**`Requests`** [tutorial](https://realpython.com/python-requests/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9T23v4X5Ri"
      },
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.get(url)\n",
        "text = resp.text \n",
        "\n",
        "# Последние 500 символов. Аккуратно! Спойлеры!\n",
        "print(text[-500:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNv4-lX9CusR"
      },
      "source": [
        "resp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IkB1iNlX5Rk"
      },
      "source": [
        "## 1. Предобработка\n",
        "\n",
        "Теперь нам надо его немного предобработать.  Пусть все слова пишутся с маленькой буквы. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0idm7hLjX5Rk"
      },
      "source": [
        "text = text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Asoq64X5Rl"
      },
      "source": [
        "Разобьём весь текст на предложения. \\\n",
        "**`re`** [tutorial](https://tproger.ru/translations/regular-expression-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYZEyG_X5Rl"
      },
      "source": [
        "import re \n",
        "# выкидываем лишние символы! \n",
        "text = re.sub('\\n|\\t|\\r', ' ', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lWzXCmsPnTg"
      },
      "source": [
        "**`nltk`** [tutorial](https://www.guru99.com/nltk-tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFrHan9tOykI"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# nltk.download('all') # если хотим всё и сразу \n",
        "\n",
        "# нам хватит вот этого: \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVq7vAJ-X5Rm"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sents = sent_tokenize(text)\n",
        "\n",
        "len(sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLcWiRIXX5Rn"
      },
      "source": [
        "sents[220]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOGapnVX5Rn"
      },
      "source": [
        "Разобьём каждое предложение на отдельные слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfBgyxv8Uo7p"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(word_tokenize(sents[0]), '\\n')\n",
        "sents[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLH0sapIX5Ro"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "tokenizer.tokenize(sents[220])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdtI_QmvX5Ro"
      },
      "source": [
        "# разбейте все предложения на токены \n",
        "sents_tokenize  =  [tokenizer.tokenize(item) for item in sents]\n",
        "sents_tokenize[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grHmUh9HX5Rp"
      },
      "source": [
        "# Flatten без numpy :) \n",
        "words = [item for sent in sents_tokenize for item in sent]\n",
        "words[:10]\n",
        "\n",
        "# Конструкция выше аналогично этому\n",
        "# words = []\n",
        "# for sent in sents_tokenize:\n",
        "#     for item in sent:\n",
        "#         if True:\n",
        "#             words.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwLIrq0X5Rp"
      },
      "source": [
        "len(words) # всего слов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx5aIgXwX5Rq"
      },
      "source": [
        "len(set(words)) # уникальных слов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNzy4bkZX5Rq"
      },
      "source": [
        "Можно выбросить все стоп-слова. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIbhM3_AX5Rr"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_ru = stopwords.words('russian') \n",
        "stopwords_ru[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPBGw8oQX5Rr"
      },
      "source": [
        "len(stopwords_ru)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zioQwW9zX5Rs"
      },
      "source": [
        "# избавьтесь от стоп-слов \n",
        "sents_tokenize = [[item for item in sent if item not in stopwords_ru]\n",
        "                        for sent in sents_tokenize]\n",
        "sents_tokenize[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uohGUT4rX5Rs"
      },
      "source": [
        "Слов в корпусе не очень много. Давайте лемматизируем их.  В этом нам поможет библиотека **pymorphy2.**\n",
        "\n",
        "**pymorphy2** — это полноценный морфологический анализатор, целиком написанный на Python. Он также умеет ставить слова в нужную форму (спрягать и склонять). [Документация по pymorphy2.](https://pymorphy2.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCfxEO2DYuak"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3VlaVAKX5Rs"
      },
      "source": [
        "import pymorphy2\n",
        "\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "text = \"Филипп пошёл в Авиньон и пленил пап!\"\n",
        "tokens = tokenizer.tokenize(text)  # regexp tokenizer initialized earlier\n",
        "\n",
        "\" \".join(morph.normal_forms(token)[0] for token in tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50_736blX5Rt"
      },
      "source": [
        "p = morph.parse('стали')\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl-0CKFZZ9cs"
      },
      "source": [
        "Зачем нужно возвращать словарь в такой конструкции: `morph.normal_forms(token)[0]`? Ответ ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdW7pm_lX5Rt"
      },
      "source": [
        "morph.normal_forms('стали')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je5kur5hX5Ru"
      },
      "source": [
        "Обработаем все слова из датасета. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z0NGwleX5Ru"
      },
      "source": [
        "# лемматизируйте все слова из датасета\n",
        "sents_tokenize = [[morph.normal_forms(item)[0] for item in sent] \n",
        "                    for sent in sents_tokenize]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOh4EV4DX5Ru"
      },
      "source": [
        "# Flatten без numpy :) \n",
        "words = [item for sent in  sents_tokenize for item in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekL99qSoX5Rv"
      },
      "source": [
        "len(words) # всего слов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gsF0hBGX5Rv"
      },
      "source": [
        "len(set(words)) # уникальных слов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLdPZ_nsX5Rw"
      },
      "source": [
        "Хватит обработок! Мы тут не анализом текстов занимаемся, а нейросетками. Если хочется больше предобработки, хороший мануал по [ссылке](https://nbviewer.jupyter.org/github/FUlyankin/hse_texts_do/blob/master/sem_1/texts_sem1.ipynb).  Давайте построим словарик с частотностями и перейдём к моделированию. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XydRhjpLX5Rw"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_dict = Counter(words)\n",
        "word_dict.most_common()[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gx9vmAVX5Rx"
      },
      "source": [
        "words = word_dict.most_common()\n",
        "len([item for item in words if item[1] >= 3])  # совсем мало :) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQPu4NRWX5Rx"
      },
      "source": [
        "## 2. Моделирование\n",
        "\n",
        "__Основные параметры:__\n",
        "\n",
        "* данные должны быть итерируемым объектом \n",
        "* `vector_size` — размер вектора, \n",
        "* `window` — размер окна наблюдения,\n",
        "* `min_count` — мин. частотность слова в корпусе,\n",
        "* `sg` — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
        "* `sample` — порог для downsampling'a высокочастотных слов,\n",
        "* `workers` — количество потоков,\n",
        "* `alpha` — learning rate,\n",
        "* `epochs` — количество эпох обучения,\n",
        "* `max_vocab_size` — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение привышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
        "\n",
        "\n",
        "**`gensim`** [documentation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)\\\n",
        "[Нововведения](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) в **gensim 4.0**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2iURrtzD2nb"
      },
      "source": [
        "!pip install gensim==4.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8w7ft6qHImx"
      },
      "source": [
        "type(sents_tokenize), len(sents_tokenize)\n",
        "# sents_tokenize[0][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaoWjIeVX5Ry"
      },
      "source": [
        "#%%time \n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# vector_size - размерность векторов, которые мы хотим обучить\n",
        "# window - ширина окна контекста\n",
        "# min_count - если слово встречается реже, для него не учим модель\n",
        "model = Word2Vec(vector_size=100, window=2, min_count=3, workers=-1)\n",
        "\n",
        "# строительство словаря, чтобы обучение шло быстрее\n",
        "model.build_vocab(sents_tokenize)\n",
        "\n",
        "# обучение модели (еще так можно модель дообучать)\n",
        "# первый аргумент - наша выборка, генератор будет вкидывать в модель наши тексты, пока они не кончатся\n",
        "# второй аргумент - число примеров в выборке \n",
        "# третий аргумент - количество эпох обучения: сколько раз модель пройдётся по всему корпусу текстов\n",
        "model.train(sents_tokenize, total_examples=model.corpus_count, epochs=100)\n",
        "\n",
        "# !NB в ситуации, когда у нас огромный корпус, 100 эпох это слишком много! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq1EcW4hBFr2"
      },
      "source": [
        "Другой способ инициализировать и тренировать модель.\n",
        "\n",
        "Такой синтаксис под капотом точно так же строит словарь и вызывает метод `train`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRvqvsADCzKc"
      },
      "source": [
        "model2 = Word2Vec(sentences=sents_tokenize, vector_size=100, window=2, min_count=3, workers=-1, epochs=100)\n",
        "print(model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIS2js9SX5Rz"
      },
      "source": [
        "model.corpus_count # число примеров в обучающей выборке"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW7jOL7nX5R0"
      },
      "source": [
        "Смотрим, сколько в модели слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBgsQhUTX5R0"
      },
      "source": [
        "len(model.wv), len(model2.wv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLZ-2QHJG16V"
      },
      "source": [
        "# model.wv.key_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iTK4qx5GX5R1"
      },
      "source": [
        "'старуха' in model.wv.key_to_index, 'старуха' in model2.wv.key_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddcNR0NBHI9a"
      },
      "source": [
        "model.wv.get_vecattr(\"старуха\", \"count\"), model2.wv.get_vecattr(\"старуха\", \"count\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXdJphMjX5R1"
      },
      "source": [
        "## 3. Свойства модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xtM8EnnX5R1"
      },
      "source": [
        "# вектор слова\n",
        "model.wv['старуха'][:10], model2.wv['старуха'][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cZllQCYIB2d"
      },
      "source": [
        "len(model.wv['старуха'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5CVjMM-X5R2"
      },
      "source": [
        "# размерность вектора\n",
        "model.wv['старуха'].shape, model2.wv['старуха'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK6MminRX5R4"
      },
      "source": [
        "# похожести слов \n",
        "model.wv.similarity('тварь', 'право'), model2.wv.similarity('тварь', 'право')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4V7_I3MX5R4"
      },
      "source": [
        "# самые похожие\n",
        "model.wv.most_similar('топор', topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4dcfSNjX5R5"
      },
      "source": [
        "# арифметика\n",
        "model.wv.most_similar(positive=['раскольников','соня'], \n",
        "                       negative=['тварь'])[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgusruzJUXdF"
      },
      "source": [
        "### KeyedVectors\n",
        "\n",
        "**TL;DR:** the main difference is that KeyedVectors do not support further training. On the other hand, by shedding the internal data structures necessary for training, KeyedVectors offer a smaller RAM footprint and a simpler interface.\n",
        "\n",
        "[Source](https://radimrehurek.com/gensim/models/keyedvectors.html#)\n",
        "\n",
        "\n",
        "По сути, KeyedVectors нужны для экономии места на диске при сохранении модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IKn1YaJIrPA"
      },
      "source": [
        "# model.save('aaaaa')\n",
        "\n",
        "# tmp = Word2Vec.load('aaaaa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_jXHBcBQDzu"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Store just the words + their trained embeddings.\n",
        "word_vectors = model.wv\n",
        "word_vectors.save(\"word2vec.wordvectors\")\n",
        "\n",
        "# Load back with memory-mapping = read-only, shared across processes.\n",
        "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
        "\n",
        "vector = wv['раскольников']  # Get numpy vector of a word\n",
        "vector.shape, vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDb5MW6AQasZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8QbvgviX5R5"
      },
      "source": [
        "## 4. Как дообучить модель? \n",
        "\n",
        "Ради чистоты эксперимента сохраним текущую модель и заново подгрузим её. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJcdqYTCX5R6"
      },
      "source": [
        "model_path = \"./our_w2v.model\"\n",
        "model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHhYr53X5R6"
      },
      "source": [
        "our_model = Word2Vec.load(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkha94E3X5R6"
      },
      "source": [
        "Подгрузим другое произведение и сделаем для него предобработку. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czo_1hoMX5R6"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/nevmenandr/word2vec-russian-novels/master/vector-school/SkazkaOCareSaltane.txt'\n",
        "\n",
        "resp = requests.get(url)\n",
        "text2 = resp.text \n",
        "\n",
        "# Последние 500 символов. Аккуратно! Спойлеры!\n",
        "print(text2[-500:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-SrQ-rnX5R7"
      },
      "source": [
        "Предобработка."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm_SNxg-X5R7"
      },
      "source": [
        "# А теперь ваш код предобработки\n",
        "\n",
        "sents2 = sent_tokenize(text2.lower())\n",
        "sents_tokenize2  =  [tokenizer.tokenize(item) for item in sents2]\n",
        "sents_tokenize2 = [[item for item in sent if item not in stopwords_ru]\n",
        "                        for sent in sents_tokenize2]\n",
        "                        \n",
        "# лемматизируйте все слова из датасета\n",
        "sents_tokenize2 = [[morph.normal_forms(item)[0] for item in sent] \n",
        "                    for sent in sents_tokenize2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYG1iPQoX5R8"
      },
      "source": [
        "sents_tokenize2[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-fZ6JhCX5R8"
      },
      "source": [
        "len(sents_tokenize2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUsAhqb0X5R8"
      },
      "source": [
        "Дополняем модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLhmug1mX5R8"
      },
      "source": [
        "# И теперь ваш код обучения\n",
        "model.train(corpus_iterable=sents_tokenize2, total_examples=len(sents_tokenize2), epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.corpus_count # число примеров в обучающей выборке"
      ],
      "metadata": {
        "id": "esz67rgoSFzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R4jwndmX5R8"
      },
      "source": [
        "'ядро' in model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn2SRWBRX5R9"
      },
      "source": [
        "'ядро' in our_model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xhLv-wPX5R9"
      },
      "source": [
        "# our_model.wv.most_similar('ядро')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSLPmxKX5R9"
      },
      "source": [
        "Пример со старым словом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AThOHD0GX5R9"
      },
      "source": [
        "our_model.wv.most_similar('сын')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF2Q35ViX5R9"
      },
      "source": [
        "model.wv.most_similar('сын')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVHk4adVX5R-"
      },
      "source": [
        "  "
      ]
    }
  ]
}